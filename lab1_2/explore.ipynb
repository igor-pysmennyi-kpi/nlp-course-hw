{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "# from langdetect import detect\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<spacy_fastlang.LanguageDetector at 0x79f2e228f010>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy_fastlang import LanguageDetector\n",
    "import uk_core_news_trf\n",
    "\n",
    "# nlp = spacy.load(\"uk_core_news_trf\")\n",
    "nlp = spacy.load('uk_core_news_sm')\n",
    "if 'language_detector' in nlp.pipe_names:\n",
    "    nlp.remove_pipe('language_detector')\n",
    "\n",
    "# Add the new language detector to the pipeline\n",
    "language_detector = LanguageDetector()\n",
    "nlp.add_pipe('language_detector')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "view  = []\n",
    "with open('data/books.jsonlines', 'r') as f:\n",
    "    for line in f:\n",
    "        data = json.loads(line)\n",
    "        description_doc = nlp(data['description'])\n",
    "        if not data['description'].strip() or sum(c.isalpha() for c in data['description']) < 3: \n",
    "            logging.debug(f\"Skipping book {data['book_id']} because it has no description\") \n",
    "            continue\n",
    "        item = {}\n",
    "        item['book_id'] = data['book_id']\n",
    "        item['title'] = data['title']\n",
    "        item['description'] = \" \".join(token.text for token in description_doc if not token.is_punct and not token.is_stop and not token.is_space)\n",
    "        item['language'] = description_doc._.language\n",
    "\n",
    "        print(item)            \n",
    "        view.append(item)\n",
    "\n",
    "df = pd.DataFrame(view)\n",
    "df.to_parquet('data/books.parquet', index=False)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        book_id                                              title  \\\n",
      "0             0                             Їсти, молитися, кохати   \n",
      "1             1  Серія книжок про Джуді Муді (комплект із 10 книг)   \n",
      "2             2                   Три новеллы (комплект из 2 книг)   \n",
      "3             3  Тренажер мозга. Как развить гибкость мышления ...   \n",
      "4             4                               У пошуках Івана Сили   \n",
      "...         ...                                                ...   \n",
      "241984   241984                               Maps for Lost Lovers   \n",
      "241985   241985                                    The Round House   \n",
      "241986   241986                                Diary of a Somebody   \n",
      "241987   241987                            The Twenty-Seventh City   \n",
      "241988   241988                                   Children of Ruin   \n",
      "\n",
      "                                              description  \n",
      "0       «Їсти, молитися, кохати» - надзвичайно щира ро...  \n",
      "1       На вас чекає знайомство з кумедною дівчинкою н...  \n",
      "2       Новели Фредріка Бакмана пройняті тим же м'яким...  \n",
      "3       Ви - це ваш мозок. Ваш мозок має неймовірний п...  \n",
      "4       Документальні нариси Олександра Гавроша, автор...  \n",
      "...                                                   ...  \n",
      "241984                                                     \n",
      "241985                                                     \n",
      "241986   - \\nPart tender love story, part murder myste...  \n",
      "241987  Сучасний американський письменник, якого деякі...  \n",
      "241988  Thousands of years ago, Earth’s terraforming p...  \n",
      "\n",
      "[241989 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Read the Parquet file\n",
    "# df = pd.read_parquet('data/books.parquet')\n",
    "# Print the DataFrame\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'>' not supported between instances of 'NoneType' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\n",
      "\u001b[0;31mTypeError\u001b[0m: '>' not supported between instances of 'NoneType' and 'int'"
     ]
    }
   ],
   "source": [
    "None >3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'book_id': 0, 'review_id': 0, 'title': 'Сердечна книга!', 'is_positive': True, 'description': 'Добралася відомого роману вийшов українською мовою Знаєте отримувала такого пристрасного головою зануреного бірюзово океанічного задоволення читання Книга захопила перших сторінок тримала останньої ниткою серця серце близькою розповідь авторки відображає власну історію життя закоханість якою Елізабет простеляє килим речень крізь подорож континентам островам власної душі книга йшла рук Напрочуд легко написана бурхливо перетікає сердечним рікам робота Щиро справжньому відкриті дитячі долоні книга показує життя жінки віддзеркалює мільйоном промінчиків життя кожної Читаючи книгу слова відгукувалися посмішкою грішне смішне сльозами зіжмакане викинуте назавжди зануренням тунель минулого залишеним смарагдом шовковою стежкою майбутнє смолоскипом кричала щасливе живе Безмежно вдячна Елізабет прожите задоволення читачка Захоплююся сим творінням письменниця Сердечно рекомендую читання Ірина Запотоцька', 'language': 'uk'}\n"
     ]
    }
   ],
   "source": [
    "view  = []\n",
    "with open('data/reviews.jsonlines', 'r') as f:\n",
    "    for line in f:\n",
    "        data = json.loads(line)\n",
    "        description_doc = nlp(data['review'])\n",
    "        if not data['review'].strip() or sum(c.isalpha() for c in data['review']) < 3: \n",
    "            logging.debug(f\"Skipping review {data['review']} because it has no content\") \n",
    "            continue\n",
    "        item = {}\n",
    "        item['book_id'] = data['book_id']\n",
    "        item['review_id'] = data['book_id']\n",
    "        item['title'] = data['reviewTitle']\n",
    "        item['is_positive'] = data.get('rating',0) > 3\n",
    "        item['description'] = \" \".join(token.text for token in description_doc if not token.is_punct and not token.is_stop and not token.is_space)\n",
    "        item['language'] = description_doc._.language\n",
    "\n",
    "        print(item)            \n",
    "        view.append(item)\n",
    "\n",
    "df = pd.DataFrame(view)\n",
    "df.to_parquet('data/reviews.parquet', index=False)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the Parquet file into a DataFrame\n",
    "df_reviews = pd.read_parquet('data/reviews.parquet')\n",
    "positive_reviews = df_reviews[df_reviews['is_positive'] == True].value_counts('book_id').sort_index(ascending=True)\n",
    "max_reviews = positive_reviews.max()\n",
    "\n",
    "df_books = pd.read_parquet('data/books.parquet')\n",
    "df_books = df_books.merge(positive_reviews, on='book_id', how='left')\n",
    "df_books = df_books.rename(columns={'count': 'positive_reviews_count'})\n",
    "df_books = df_books.fillna({'positive_reviews_count': 0})\n",
    "df_books['positive_reviews_score'] = df_books['positive_reviews_count'] / max_reviews\n",
    "\n",
    "df_books.to_parquet('data/books_enriched.parquet', index=False)   \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
